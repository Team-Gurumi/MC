import json
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta, timezone

LOG_DIR = "./logs"
N_TASKS = 1000
HEARTBEAT_INTERVAL = 5  # ì´ˆ
LEASE_TTL = 15          # ì´ˆ

def parse_log_file(file_path):
    """ë‹¨ì¼ ë¡œê·¸ íŒŒì¼ì—ì„œ JSON ë¼ì¸ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    records = []
    with open(file_path, 'r') as f:
        for line in f:
            try:
                # Goì˜ í‘œì¤€ ë¡œê·¸ í¬ë§· (ë‚ ì§œ ì‹œê°„ {json})ì„ ì²˜ë¦¬
                if '{' in line:
                    json_str = line[line.find('{'):]
                    records.append(json.loads(json_str))
            except json.JSONDecodeError:
                continue # JSONì´ ì•„ë‹Œ ë¡œê·¸ ë¼ì¸ì€ ë¬´ì‹œ
    return records

def analyze_logs():
    """ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ ì§€í‘œë¥¼ ê³„ì‚°í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    all_logs = []
    for filename in os.listdir(LOG_DIR):
        if filename.startswith("agent_") and filename.endswith(".log"):
            all_logs.extend(parse_log_file(os.path.join(LOG_DIR, filename)))

    if not all_logs:
        print("ë¶„ì„í•  ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = pd.DataFrame(all_logs)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df = df.sort_values(by='timestamp').reset_index(drop=True)

    # 1. ì™„ë£Œ ì„±ê³µë¥  ê³„ì‚°
    completed = df[df['event'] == 'task_completed']
    succeeded = completed[completed['status'] == 'succeeded']
    success_rate = len(succeeded) / N_TASKS if N_TASKS > 0 else 0

    # 2. MTTR ê³„ì‚°
    killed_agents = df[df['event'] == 'agent_killed']
    
    # ì´ ì‹¤í—˜ì—ì„œëŠ” SIGKILLì„ ì‚¬ìš©í•˜ë¯€ë¡œ, ì‹¤ì œ 'lease_expired' ë¡œê·¸ëŠ” ì—†ìŠµë‹ˆë‹¤.
    # ëŒ€ì‹ , ë§ˆì§€ë§‰ í•˜íŠ¸ë¹„íŠ¸ì™€ TTLì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë£Œ ì‹œê°ì„ ì¶”ì •í•©ë‹ˆë‹¤.
    mttr_values = []
    
    # ì¢…ë£Œëœ ì—ì´ì „íŠ¸ê°€ ì†Œìœ í–ˆë˜ ì‘ì—…ì„ ì°¾ìŠµë‹ˆë‹¤.
    tasks_of_killed_agents = df[df['agent_id'].isin(killed_agents['agent_id'].unique()) & df['task_id'].notna()]
    
    for task_id, group in tasks_of_killed_agents.groupby('task_id'):
        last_heartbeat = group[group['event'] == 'lease_heartbeat_sent'].tail(1)
        
        # ì¬ì‹œì‘(ë‹¤ì‹œ running ìƒíƒœê°€ ëœ) ì´ë²¤íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        restarts = df[(df['task_id'] == task_id) & (df['event'] == 'task_running') & 
                      ~df['agent_id'].isin(killed_agents['agent_id'].unique())]

        if not last_heartbeat.empty and not restarts.empty:
            t_last_hb = last_heartbeat['timestamp'].iloc[0]
            t_expire = t_last_hb + timedelta(seconds=LEASE_TTL)
            
            # ë§Œë£Œ ì´í›„ì˜ ì²« ì¬ì‹œì‘ì„ ì°¾ìŠµë‹ˆë‹¤.
            first_restart = restarts[restarts['timestamp'] > t_expire].head(1)
            if not first_restart.empty:
                t_restart = first_restart['timestamp'].iloc[0]
                mttr = (t_restart - t_expire).total_seconds()
                mttr_values.append(mttr)


    # 3. ì¤‘ë³µ ì‹¤í–‰ë¥  ê³„ì‚°
    running_events = df[df['event'] == 'task_running'].copy()
    running_events = running_events.drop_duplicates(subset=['task_id', 'agent_id']) # ë™ì¼ ì—ì´ì „íŠ¸ì˜ ì¤‘ë³µ ì‹¤í–‰ ë¡œê·¸ ì œê±°
    
    duplicate_tasks = 0
    for task_id, group in running_events.groupby('task_id'):
        if len(group) > 1:
            # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
            group = group.sort_values(by='timestamp')
            timestamps = group['timestamp'].to_list()
            for i in range(len(timestamps) - 1):
                # TTL/2 (7.5ì´ˆ) ì´ë‚´ì— ë‹¤ë¥¸ ì—ì´ì „íŠ¸ê°€ ì‹¤í–‰í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
                if (timestamps[i+1] - timestamps[i]).total_seconds() <= LEASE_TTL / 2:
                    duplicate_tasks += 1
                    break # í•œ ì‘ì—…ë‹¹ í•œ ë²ˆë§Œ ì¹´ìš´íŠ¸

    duplicate_rate = duplicate_tasks / N_TASKS if N_TASKS > 0 else 0
    
    # 4. ì˜¤íƒ íšŒìˆ˜ìœ¨ (ì´ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” 0ì´ ì˜ˆìƒë¨)
    # "lease_acquired" ì´ë²¤íŠ¸ ì¤‘, ì´ì „ ì†Œìœ ìê°€ ìˆì—ˆë˜ ê²½ìš°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    lease_acquired = df[df.event == 'lease_acquired'].sort_values('timestamp')
    reassigned_tasks = lease_acquired[lease_acquired.duplicated('task_id', keep='first')]
    
    false_recoveries = 0
    for _, row in reassigned_tasks.iterrows():
        task_id = row['task_id']
        reassign_time = row['timestamp']
        
        # ì´ì „ lease ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        prev_leases = lease_acquired[(lease_acquired['task_id'] == task_id) & (lease_acquired['timestamp'] < reassign_time)]
        if prev_leases.empty:
            continue
            
        last_owner_id = prev_leases.iloc[-1]['agent_id']
        last_heartbeats = df[(df['task_id'] == task_id) & (df['agent_id'] == last_owner_id) & (df['event'] == 'lease_heartbeat_sent')]
        
        if last_heartbeats.empty:
            continue

        t_last_hb = last_heartbeats.iloc[-1]['timestamp']
        t_expire_estimated = t_last_hb + timedelta(seconds=LEASE_TTL)

        # ì¬ì„ ì ì´ ì¶”ì •ëœ ë§Œë£Œ ì‹œê° ì´ì „ì— ë°œìƒí–ˆë‹¤ë©´ ì˜¤íƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ë˜í•œ, ë§ˆì§€ë§‰ í•˜íŠ¸ë¹„íŠ¸ê°€ ë§Œë£Œ ì§ì „(HB * 1.2 ì´ë‚´)ì— ìˆì—ˆë‹¤ë©´ ì˜¤íƒ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
        if reassign_time < t_expire_estimated and (t_expire_estimated - t_last_hb).total_seconds() < HEARTBEAT_INTERVAL * 1.2:
            false_recoveries += 1
            
    false_recovery_rate = false_recoveries / N_TASKS if N_TASKS > 0 else 0


    # --- ê²°ê³¼ ì¶œë ¥ ---
    print("\n--- ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ---")
    print(f"âœ… ì™„ë£Œ ì„±ê³µë¥ : {success_rate:.4%} ({len(succeeded)}/{N_TASKS})")
    
    if mttr_values:
        median_mttr = np.median(mttr_values)
        p95_mttr = np.percentile(mttr_values, 95)
        print(f"â±ï¸ MTTR ì¤‘ì•™ê°’: {median_mttr:.2f}s")
        print(f"â±ï¸ MTTR p95: {p95_mttr:.2f}s")
    else:
        print("â±ï¸ MTTR: ê³„ì‚°ëœ ë°ì´í„° ì—†ìŒ (ë³µêµ¬ëœ ì‘ì—… ì—†ìŒ)")

    print(f"ğŸ‘¯ ì¤‘ë³µ ì‹¤í–‰ë¥ : {duplicate_rate:.4%} ({duplicate_tasks}/{N_TASKS})")
    print(f"ğŸ¤” ì˜¤íƒ íšŒìˆ˜ìœ¨: {false_recovery_rate:.4%} ({false_recoveries}/{N_TASKS})")
    
    # --- ê·¸ë˜í”„ ìƒì„± ---
    if mttr_values:
        # íˆìŠ¤í† ê·¸ë¨
        plt.figure(figsize=(10, 6))
        plt.hist(mttr_values, bins=30, edgecolor='black', alpha=0.7)
        plt.axvline(median_mttr, color='red', linestyle='dashed', linewidth=2, label=f'Median: {median_mttr:.2f}s')
        plt.axvline(p95_mttr, color='orange', linestyle='dashed', linewidth=2, label=f'p95: {p95_mttr:.2f}s')
        plt.title('MTTR Distribution')
        plt.xlabel('Time to Recover (seconds)')
        plt.ylabel('Frequency')
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.savefig('mttr_histogram.png')
        print("\nğŸ“ˆ MTTR íˆìŠ¤í† ê·¸ë¨ì´ 'mttr_histogram.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # CDF
        plt.figure(figsize=(10, 6))
        sorted_mttr = np.sort(mttr_values)
        yvals = np.arange(len(sorted_mttr)) / float(len(sorted_mttr))
        plt.plot(sorted_mttr, yvals)
        plt.axvline(median_mttr, color='red', linestyle='dashed', linewidth=1, label=f'Median: {median_mttr:.2f}s')
        plt.axvline(p95_mttr, color='orange', linestyle='dashed', linewidth=1, label=f'p95: {p95_mttr:.2f}s')
        plt.title('MTTR Cumulative Distribution Function (CDF)')
        plt.xlabel('Time to Recover (seconds)')
        plt.ylabel('Cumulative Probability')
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.legend()
        plt.savefig('mttr_cdf.png')
        print("ğŸ“ˆ MTTR CDF ê·¸ë˜í”„ê°€ 'mttr_cdf.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    analyze_logs()
